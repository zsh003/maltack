## 4.2 基于多分支 CNN 的恶意 PE 识别模型构建

为增强基于 CNN 的直方图分类器在 Stacking 集成学习中的互补性，本研究提出了一种结合模型集成与特征增强的优化方案。该方案通过引入多分支 CNN 架构、调整输出层设计以及优化训练策略，显著提升了基模型的泛化能力与特征表达多样性。

### 4.2.1 特征引入与网络输入

本模型输入统一512维向量，前256维为全局字节直方图特征，后256维为局部熵直方图特征。直方图数据在一维上具有局部相关性，适于应用卷积网络提取空间模式，因此选择CNN架构，参考 LeCun 等。在输入前，向量被重塑为（32,16,1）的单通道伪图像，以保持二维邻接信息。



表 4‑1 PE直方图特征（512维）

| **特征类别**         | **维度** | **特征描述**              |
| -------------------- | -------- | ------------------------- |
| ByteHistogram        | 256      | 各字节（0-255）的出现频率 |
| ByteEntropyHistogram | 256      | 滑动窗口熵值分布          |



同时，在原有 512 维直方图特征的基础上，引入动态特征增强机制。具体而言，将输入的二维伪图像（32x16x1）通过随机裁剪（RandomCrop）与翻转（RandomFlip）生成多版本训练样本，增强模型对局部特征的鲁棒性。同时，参考 Saxe 等提出的二维字节熵直方图方法，将原始特征与滑动窗口熵特征进行级联，形成混合特征空间（512 维 + 256 维）。这种设计既保留了全局字节分布模式，又强化了局部熵的空间关联，为 Stacking 提供更丰富的元特征。



### 4.2.2 网络结构与超参数

为提升模型的特征表达多样性，采用**多分支 CNN 架构**（图 4.2）。在原有两级卷积块基础上，新增两条并行分支：

#### （1） 注意力分支

引入通道注意力模块（Squeeze-and-Excitation，SE），通过全局平均池化与全连接层动态调整特征通道权重，增强关键模式的捕捉能力。

 

#### （2） 残差分支

添加跳跃连接（Skip Connection），将浅层特征直接传递至深层，缓解梯度消失问题，提升模型对细节特征的敏感度。

数学上，改进后的网络结构可表示为：

$ 
H_{l}^{\text{attn}} = \text{SE}\left( \text{Pool}\left( \phi\left( W_{l} * H_{l-1} + b_{l} \right) \right) \right)
 $

$ 
H_{l}^{\text{res}} = H_{l-1} + \text{Pool}\left( \phi\left( W_{l} * H_{l-1} + b_{l} \right) \right)
 $

$ 
H_{l} = \text{Concat}\left( H_{l}^{\text{attn}}, H_{l}^{\text{res}} \right)
 $

其中，SE 模块通过学习通道间的依赖关系，自适应地分配特征权重，而残差连接确保了信息的跨层流动。

### 4.2.3 训练策略与优化配置

采用BinaryCrossentropy损失函数与Adam优化器，设置初始学习率为0.001, $\beta_1=0.9,\beta_2=0.999$ 。引入EarlyStopping(patience=6) 以防过拟合，并使用ReduceLROnPlateau(patience=4, factor=0.5) 动态调整学习率。训练批次大小设为128，共训练最多50轮，最终在验证集上于第18轮触发EarlyStopping。

为适应 Stacking 对基模型的稳定性要求，引入**软标签训练**与**对抗正则化**：

#### （1）软标签生成

在训练过程中，将原始硬标签（0/1）替换为通过 K 近邻（K=5）平滑后的软标签，公式为：

$ 
   \hat{y}_{\text{soft}} = \frac{1}{K} \sum_{i=1}^{K} \text{one-hot}\left( y_{i} \right)
    $

其中，$  y_{i}  $为当前样本的 K 个最近邻标签。软标签训练可降低模型对噪声的敏感性，提升输出概率的可靠性。

#### （2）对抗训练

采用 FGSM（Fast Gradient Sign Method）生成对抗样本，在训练过程中交替优化原始样本与对抗样本的损失，增强模型的鲁棒性。

此外，调整学习率策略为**余弦退火**（Cosine Annealing），动态调整学习率以平衡收敛速度与泛化能力。

### 4.2.4 消融实验与主成分对比

为验证优化方案的有效性，设计了三组对比实验：

**基础模型**：原有 CNN 架构（表 4.2）。

**多分支模型**：新增注意力与残差分支。

**增强训练模型**：结合软标签与对抗训练。

实验结果显示（表 4.3）：

多分支模型在测试集上的 AUC 提升至 0.985，F1 分数提高 0.961，表明特征表达多样性增强。

增强训练模型的泛化能力显著提升，在测试样本中的 Recall 由 0.965 增至 0.972，验证了软标签与对抗训练的有效性。

### 

优化后的 CNN 模型在测试集上取得 AUC=0.985、F1=0.961 的性能提升，尤其在以下方面表现突出：

**特征互补性**：多分支架构通过注意力与残差连接，有效捕捉了字节分布的全局模式与局部细节，为 Stacking 提供了更丰富的元特征。

**鲁棒性**：对抗训练显著提升了模型对噪声的抵抗能力，在特征扰动场景下误报率降低 1.2%。

**输出稳定性**：软标签训练使模型输出概率分布更平滑，与其他基模型（如 LightGBM）的相关性降低 0.15，增强了 Stacking 的集成效果。

